# -*- coding: utf-8 -*-
"""Syngrid_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNgONYBZpRuwjaa4xwrqiTSevgU0pMwj
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from collections import deque
import re
import os
from sentence_transformers import SentenceTransformer

PRIORITY_PAGES = [
    "", "about", "services", "solutions", "products", "contact", "team",
    "careers", "blog", "case-studies", "portfolio", "industries",
    "technology", "expertise", "what-we-do", "who-we-are", "footer"
]


class CustomEmbeddings:
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts):
        if not texts:
            return []
        embeddings = self.model.encode(
            texts,
            normalize_embeddings=True,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        return embeddings.tolist()

    def embed_query(self, text):
        embedding = self.model.encode(
            [text],
            normalize_embeddings=True,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        return embedding[0].tolist()


class SyngridAI:
    def __init__(self, openrouter_key: str, model: str):
        self.retriever = None
        self.cache = {}
        self.status = {"ready": False, "pages_scraped": 0}
        self.scraped_content = {}
        self.company_info = {
            'emails': set(), 'phones': set(),
            'india_address': None, 'singapore_address': None
        }
        self.openrouter_key = openrouter_key
        self.model = model
        self.api_base = "https://openrouter.ai/api/v1/chat/completions"

    def clean_address(self, text):
        text = ' '.join(text.split())
        text = re.sub(r'(Corporate Office|Branch Office|Head Office|Registered Office)', '', text, flags=re.IGNORECASE)
        return re.sub(r'\s+', ' ', text).strip()

    def extract_contact_info(self, soup, text, url):
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b', text)
        for email in emails:
            if not email.lower().endswith(('.png', '.jpg', '.gif')):
                self.company_info['emails'].add(email.lower())

        phone_patterns = [
            r'\+91\s*\d{5}\s*\d{5}',
            r'\+\d{1,3}[-.\s]?\d{3}[-.\s]?\d{3}[-.\s]?\d{4}',
            r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}'
        ]
        for pattern in phone_patterns:
            phones = re.findall(pattern, text)
            for phone in phones:
                cleaned = re.sub(r'[^\d+]', '', phone)
                if 10 <= len(cleaned) <= 15:
                    self.company_info['phones'].add(phone.strip())

        lines = text.split('\n')
        for i, line in enumerate(lines):
            low = line.lower()
            if 'madurai' in low and '625' in line and not self.company_info['india_address']:
                block = " ".join(lines[max(0, i-2):min(len(lines), i+5)])
                cleaned = self.clean_address(block)
                if 20 < len(cleaned) < 300:
                    self.company_info['india_address'] = cleaned

            if 'singapore' in low and re.search(r'\d{6}', line) and not self.company_info['singapore_address']:
                block = " ".join(lines[max(0, i-2):min(len(lines), i+5)])
                cleaned = self.clean_address(block)
                if 20 < len(cleaned) < 300:
                    self.company_info['singapore_address'] = cleaned

    def is_valid_url(self, url, base_domain):
        try:
            parsed = urlparse(url)
            if parsed.netloc != base_domain:
                return False
            skip = [
                r'\.pdf$', r'\.jpg$', r'\.png$', r'\.gif$', r'\.zip$',
                r'/wp-admin/', r'/wp-includes/', r'/login', r'/register',
                r'/cart/', r'/checkout/', r'/feed/', r'/rss/'
            ]
            return not any(re.search(pattern, url.lower()) for pattern in skip)
        except:
            return False

    def extract_content(self, soup, url):
        content_dict = {'url': url, 'title': '', 'main_content': '', 'metadata': {}}

        try:
            t = soup.find('title')
            if t:
                content_dict['title'] = t.get_text(strip=True)

            meta = soup.find('meta', attrs={"name": "description"})
            if meta and meta.get("content"):
                content_dict['metadata']['description'] = meta["content"]

            full_text = soup.get_text(separator="\n", strip=True)
            self.extract_contact_info(soup, full_text, url)

            for tag in soup(['script', 'style', 'nav', 'aside', 'iframe', 'noscript', 'form']):
                tag.decompose()

            content_selectors = [
                "main", "article", "[role='main']", ".content",
                ".main-content", "#content", "#main"
            ]
            parts = []
            for sel in content_selectors:
                parts.extend(soup.select(sel))

            main_content = soup.find("body") if not parts else soup.new_tag("div")
            if parts:
                for p in parts:
                    main_content.append(p)

            if main_content:
                text = main_content.get_text(separator="\n", strip=True)
                lines = [l.strip() for l in text.split("\n") if len(l.strip()) > 20]
                content_dict['main_content'] = "\n".join(lines)
        except Exception as e:
            print(f"Content extraction error for {url}: {str(e)}")

        return content_dict

    def scrape_website(self, base_url, max_pages=40):
        visited = set()
        all_content = []
        q = deque()
        base_domain = urlparse(base_url).netloc

        for p in PRIORITY_PAGES:
            q.append(urljoin(base_url, p))

        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}

        while q and len(visited) < max_pages:
            url = q.popleft().split("#")[0].split("?")[0]

            if url in visited or not self.is_valid_url(url, base_domain):
                continue

            try:
                r = requests.get(url, headers=headers, timeout=15)
                if r.status_code != 200:
                    continue

                visited.add(url)

                soup = BeautifulSoup(r.text, "html.parser")
                data = self.extract_content(soup, url)

                if len(data['main_content']) > 100:
                    formatted = f"URL: {data['url']}\nTITLE: {data['title']}\n"
                    if "description" in data['metadata']:
                        formatted += f"DESCRIPTION: {data['metadata']['description']}\n"
                    formatted += f"\nCONTENT:\n{data['main_content']}"
                    all_content.append(formatted)
                    self.scraped_content[url] = data

                for link in soup.find_all("a", href=True):
                    next_url = urljoin(url, link['href']).split("#")[0].split("?")[0]
                    if next_url not in visited and self.is_valid_url(next_url, base_domain):
                        q.append(next_url)

            except Exception:
                continue

        self.status["pages_scraped"] = len(visited)

        if self.company_info['emails'] or self.company_info['phones']:
            header = "COMPANY CONTACT INFORMATION\n" + "="*50 + "\n\n"
            if self.company_info['india_address']:
                header += f"INDIA OFFICE:\n{self.company_info['india_address']}\n\n"
            if self.company_info['singapore_address']:
                header += f"SINGAPORE OFFICE:\n{self.company_info['singapore_address']}\n\n"
            for e in sorted(self.company_info['emails']):
                header += f"Email: {e}\n"
            for p in sorted(self.company_info['phones']):
                header += f"Phone: {p}\n"
            all_content.insert(0, header)

        return ("\n\n" + "="*80 + "\n\n").join(all_content)

    def get_company_contact_info(self):
        info = self.company_info
        if not any([info['emails'], info['phones'], info['india_address'], info['singapore_address']]):
            return "No contact details found."

        msg = "COMPANY CONTACT INFORMATION\n\n"
        if info['india_address']:
            msg += f"India Office:\n{info['india_address']}\n\n"
        if info['singapore_address']:
            msg += f"Singapore Office:\n{info['singapore_address']}\n\n"
        if info['emails']:
            msg += "Emails:\n" + "\n".join([f"• {e}" for e in info['emails']]) + "\n\n"
        if info['phones']:
            msg += "Phones:\n" + "\n".join([f"• {p}" for p in info['phones']]) + "\n"

        return msg.strip()

    def initialize(self, url, max_pages=40):
        try:
            content = self.scrape_website(url, max_pages)
            if len(content) < 1000:
                raise Exception("Insufficient content scraped")

            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1200,
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", " "]
            )
            chunks = splitter.split_text(content)

            if len(chunks) == 0:
                raise Exception("No text chunks created")

            embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
            embeddings = CustomEmbeddings(embedding_model)

            chroma_dir = "./syngrid_chroma"
            vectorstore = Chroma.from_texts(
                texts=chunks,
                embedding=embeddings,
                persist_directory=chroma_dir
            )

            self.retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
            self.status["ready"] = True
            return True

        except Exception as e:
            print(f"Initialization Error: {str(e)}")
            return False

    def ask(self, question):
        if not self.status["ready"]:
            return "Initialization still in progress."

        q_lower = question.lower().strip()

        greetings = ["hi", "hello", "hey", "hai", "hii", "helloo", "hi there", "hello there"]
        if q_lower in greetings:
            return "Hi, I'm Syngrid AI Assistant. How can I assist you?"

        contact_words = ["email", "contact", "phone", "address", "office", "location", "reach", "call"]
        if any(k in q_lower for k in contact_words):
            return self.get_company_contact_info()

        if q_lower in self.cache:
            return self.cache[q_lower]

        try:
            docs = self.retriever.invoke(question)
            if not docs:
                return "I couldn't find relevant information in the scraped data."

            context = "\n\n".join([d.page_content for d in docs])[:4000]

            prompt = f"""Answer the question using the context below. Be concise and helpful.

Context:
{context}

Question: {question}

Answer in 2–4 sentences, focusing on the most relevant information."""

            headers = {
                "Authorization": f"Bearer {self.openrouter_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://syngrid.com",
                "X-Title": "Syngrid AI Assistant"
            }

            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant for Syngrid Technologies. Answer questions only using the given context."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 500
            }

            r = requests.post(self.api_base, headers=headers, json=payload, timeout=60)

            if r.status_code == 200:
                ans = r.json()["choices"][0]["message"]["content"].strip()
                self.cache[q_lower] = ans
                return ans
            else:
                return f"API Error ({r.status_code}): {r.text[:200]}"

        except Exception as e:
            return f"Error: {str(e)}"